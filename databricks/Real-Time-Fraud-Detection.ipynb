{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e32c61-e654-4250-a574-f07e8de8e63d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import json\n",
    "import pandas as pd\n",
    "import mlflow.pyfunc\n",
    "import pickle\n",
    "%pip install lightgbm\n",
    "import lightgbm\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# üöÄ Load Fraud Model from S3\n",
    "model_path = \"/Workspace/Users/keerthanasingi@gmail.com/lightgbm_model.pkl\"  # Update with correct path\n",
    "\n",
    "with open(model_path, \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# ‚úÖ Extract feature names safely\n",
    "try:\n",
    "    expected_features = loaded_model.feature_name()  # Use () instead of _ for Booster models\n",
    "except AttributeError:\n",
    "    expected_features = None  # Handle missing feature names\n",
    "\n",
    "# üõ†Ô∏è Get Data from Lambda (DynamoDB)\n",
    "dbutils.widgets.text(\"json_data\", \"\")\n",
    "json_data = dbutils.widgets.get(\"json_data\")\n",
    "\n",
    "if json_data:\n",
    "    data = json.loads(json_data)\n",
    "    \n",
    "    # üîπ Convert JSON to Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # ‚úÖ Ensure correct features\n",
    "    if expected_features:\n",
    "        df = df[expected_features]  # Select only model's trained features\n",
    "    else:\n",
    "        print(\"Warning: Model does not contain feature names. Using all features.\")\n",
    "\n",
    "    # üöÄ Run Fraud Prediction\n",
    "    df[\"fraud_status\"] = loaded_model.predict(df)\n",
    "\n",
    "    # üîπ Remove duplicate columns\n",
    "    #df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # üîπ Convert to Spark DataFrame\n",
    "    #spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n",
    "    #spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    # üîπ Store Results in Databricks Table\n",
    "    #spark_df.write.mode(\"append\").saveAsTable(\"cleaned_data\")\n",
    "\n",
    "    # üîπ Display Output\n",
    "    #display(spark_df)\n",
    "\n",
    "    # Capture run details\n",
    "    run_id = spark.sparkContext.getLocalProperty(\"spark.databricks.mlflow.runId\")\n",
    "    experiment_id = spark.sparkContext.getLocalProperty(\"spark.databricks.mlflow.experimentId\")\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"Experiment ID: {experiment_id}\")\n",
    "else:\n",
    "    print(\"No data provided in the json_data widget.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Real-Time-Fraud-Detection",
   "widgets": {
    "json_data": {
     "currentValue": "",
     "nuid": "341e03b4-997b-4745-bf3a-932a04daf31d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "json_data",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "json_data",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
